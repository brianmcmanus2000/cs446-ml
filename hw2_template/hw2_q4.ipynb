{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCl_aq6nsSep"
      },
      "source": [
        "**Logistic Regression Tutorial**: From Non-linear Data to Optimization\n",
        "\n",
        "This script provides a comprehensive, hands-on guide to understanding and\n",
        "implementing logistic regression for a non-linear classification problem.\n",
        "It covers feature engineering, regularization, hyperparameter tuning, and\n",
        "a comparison of different gradient descent optimization algorithms.\n",
        "\n",
        "We will use NumPy for numerical operations and Matplotlib for visualization.\n",
        "Scikit-learn is used only for generating a sample dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UZActkJsPiD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# --- Part 0: Data Generation and Visualization ---\n",
        "# We'll use scikit-learn's make_circles function to generate a dataset\n",
        "# where the data is not linearly separable. This is a classic example\n",
        "# that requires a non-linear decision boundary.\n",
        "\n",
        "X, y = make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=42)\n",
        "\n",
        "# Reshape y to be a column vector (n_samples, 1)\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "# 70% training, 15% validation, 15% testing\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.15, random_state=42\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=(0.15/0.85), random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvYi9E-6siO3"
      },
      "outputs": [],
      "source": [
        "def plot_data(X, y, title=\"Dataset\"):\n",
        "    \"\"\"Helper function to visualize the dataset.\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(X[y.flatten() == 0, 0], X[y.flatten() == 0, 1], label=\"Class 0\", alpha=0.7)\n",
        "    plt.scatter(X[y.flatten() == 1, 0], X[y.flatten() == 1, 1], label=\"Class 1\", alpha=0.7)\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    if not os.environ.get('HIDE_PLOTS', False): plt.show()\n",
        "\n",
        "print(\"--- Visualizing Initial Data ---\")\n",
        "plot_data(X, y, \"Non-linearly Separable Circular Data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq1rMClPsoqs"
      },
      "source": [
        "# --- Part 1: Logistic Regression with Non-linear Features ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YkUolhHsm7o"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\", save_fname=None):\n",
        "    \"\"\"Helper function to plot the decision boundary of a trained model.\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Plot data points\n",
        "    plt.scatter(X[y.flatten() == 0, 0], X[y.flatten() == 0, 1], label=\"Class 0\", alpha=0.7)\n",
        "    plt.scatter(X[y.flatten() == 1, 0], X[y.flatten() == 1, 1], label=\"Class 1\", alpha=0.7)\n",
        "\n",
        "    # Create a grid to evaluate the model\n",
        "    x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
        "                           np.linspace(x2_min, x2_max, 200))\n",
        "\n",
        "    # Predict on the grid\n",
        "    grid_X = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "\n",
        "    # Apply the same feature transformation used for training\n",
        "    if hasattr(model, 'poly_features') and model.poly_features:\n",
        "        grid_X_poly = model.transform(grid_X)\n",
        "    else:\n",
        "        grid_X_poly = grid_X\n",
        "\n",
        "    Z = model.predict(grid_X_poly)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "\n",
        "    # Plot the contour\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, levels=[0, 0.5, 1], colors=['#3B82F6', '#F472B6'])\n",
        "\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    if save_fname:\n",
        "      if not os.environ.get('HIDE_PLOTS', False): plt.savefig(save_fname) # Save the plot\n",
        "\n",
        "    if not os.environ.get('HIDE_PLOTS', False): plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iiNSdJAwstwW"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=10000, lambda_val=0.1, regularization=None, poly_features=False):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.lambda_val = lambda_val  # Regularization strength\n",
        "        self.regularization = regularization  # 'L1' or 'L2'\n",
        "        self.poly_features = poly_features # Flag for polynomial feature transformation\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        #TODO: (1a) Implement sigmoid function\n",
        "        # Hint: The sigmoid function is defined as 1 / (1 + e^(-z)).\n",
        "        pass\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"b. Create new features by non-linear transforms (e.g. squares)\"\"\"\n",
        "        # Original features: X1, X2\n",
        "        # New features: X1, X2, X1^2, X2^2, X1*X2\n",
        "        # Should return a np.hstack() containing X1, X2, X1^2, X2^2, X1*X2\n",
        "\n",
        "        #TODO: (1b) implement feature transform\n",
        "        # Hint: X1 is the first column of X (X[:, 0]) and X2 is the second (X[:, 1]).\n",
        "        # Create the new feature columns and then stack them horizontally with the original X.\n",
        "        # The final returned array should have 5 columns.\n",
        "        pass\n",
        "\n",
        "    def _compute_cost(self, y, y_pred):\n",
        "        m = len(y)\n",
        "        # TODO: (1c) implement cost function\n",
        "        # Hint: Implement the binary cross-entropy (log loss) cost function.\n",
        "        # Formula: J = -(1/m) * Î£ [y*log(y_pred) + (1-y)*log(1-y_pred)]\n",
        "        # Be careful about taking the log of 0.\n",
        "        cost = 0\n",
        "\n",
        "        # Add regularization term\n",
        "        reg_cost = 0\n",
        "        if self.regularization == 'L2':\n",
        "            #TODO: (2a) implement L2 Regularization\n",
        "            pass\n",
        "        elif self.regularization == 'L1':\n",
        "            #TODO: (2b) implement L1 Regularization\n",
        "            pass\n",
        "\n",
        "        return cost + reg_cost\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Apply polynomial transformation if specified\n",
        "        if self.poly_features:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros((n_features, 1))\n",
        "        self.bias = 0\n",
        "        self.costs = []\n",
        "\n",
        "        # Batch Gradient Descent\n",
        "        for i in range(self.n_iterations):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "            # Compute cost for visualization\n",
        "            cost = self._compute_cost(y, y_pred)\n",
        "            self.costs.append(cost)\n",
        "\n",
        "            # Compute gradients\n",
        "            #TODO: (1d) compute gradients\n",
        "            dw = 0\n",
        "            db = 0\n",
        "\n",
        "            # Add regularization to gradients\n",
        "            if self.regularization == 'L2':\n",
        "                dw += (self.lambda_val / n_samples) * self.weights\n",
        "            elif self.regularization == 'L1':\n",
        "                # Subgradient of L1\n",
        "                dw += (self.lambda_val / n_samples) * np.sign(self.weights)\n",
        "\n",
        "            # Update weights and bias\n",
        "            # TODO: (1a) update weights and bias\n",
        "            self.weights -= 0\n",
        "            self.bias -= 0\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Cost after iteration {i}: {cost:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Note: The model expects the same feature transformation as during training.\n",
        "        # The plotting function handles this transformation.\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self._sigmoid(linear_model)\n",
        "        return (y_pred > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lCM1gIUsvml"
      },
      "outputs": [],
      "source": [
        "# 1.a. Attempt to fit with a simple linear model\n",
        "linear_model = LogisticRegression(learning_rate=0.1, n_iterations=20000)\n",
        "linear_model.fit(X_train, y_train)\n",
        "plot_decision_boundary(linear_model, X, y, \"Linear Decision Boundary (Fails)\", \"submission_4a_linear_decision_boundary.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6dEygXwsyW6"
      },
      "outputs": [],
      "source": [
        "nonlinear_model = LogisticRegression(learning_rate=0.1, n_iterations=20000, poly_features=True)\n",
        "nonlinear_model.fit(X_train, y_train)\n",
        "plot_decision_boundary(nonlinear_model, X, y, \"Non-linear Decision Boundary (Succeeds)\", \"submission_4a_nonlinear_decision_boundary.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swX5Gr_1s4f6"
      },
      "source": [
        "# --- Part 2: L1 vs. L2 Regularization ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN4c9-oFxYKH"
      },
      "source": [
        "Regularization helps prevent overfitting by penalizing large weights.\n",
        "\n",
        "Overfitting occurs when a model learns the training data too well, including the noise and outliers, leading to poor performance on unseen data. Large weights in a logistic regression model can cause the decision boundary to be too complex and highly sensitive to small changes in the input features, which is a sign of overfitting.\n",
        "\n",
        "Regularization adds a penalty term to the cost function that is minimized during training. This penalty is based on the magnitude of the weights. By adding this penalty, the model is encouraged to keep the weights small.\n",
        "\n",
        "*   **L1 Regularization (Lasso):** Adds the sum of the absolute values of the weights to the cost function. This encourages sparsity, meaning it can drive some weights exactly to zero, effectively performing feature selection.\n",
        "*   **L2 Regularization (Ridge):** Adds the sum of the squared values of the weights to the cost function. This shrinks the weights towards zero but rarely makes them exactly zero. L2 regularization is good at preventing multicollinearity (high correlation between features).\n",
        "\n",
        "By limiting the size of the weights, regularization makes the model simpler and less prone to fitting the noise in the training data, thus improving its generalization ability on new data.\n",
        "\n",
        "Implement the L1 and L2 regularization above to complete the following section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl7c_6zis7M_"
      },
      "outputs": [],
      "source": [
        "print(\"Training L2 (Ridge) Regularized Model...\")\n",
        "l2_model = LogisticRegression(learning_rate=0.1, n_iterations=20000,\n",
        "                              regularization='L2', lambda_val=0.5, poly_features=True)\n",
        "l2_model.fit(X_train, y_train)\n",
        "plot_decision_boundary(l2_model, X, y, \"Decision Boundary with L2 Regularization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iefpywnstBjD"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTraining L1 (Lasso) Regularized Model...\")\n",
        "l1_model = LogisticRegression(learning_rate=0.1, n_iterations=20000,\n",
        "                              regularization='L1', lambda_val=0.5, poly_features=True)\n",
        "l1_model.fit(X_train, y_train)\n",
        "plot_decision_boundary(l1_model, X, y, \"Decision Boundary with L1 Regularization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buoSUIQjtFDK"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Comparing Model Weights ---\")\n",
        "print(f\"L2 Model Weights:\\n{l2_model.weights.flatten()}\")\n",
        "print(f\"L1 Model Weights:\\n{l1_model.weights.flatten()}\")\n",
        "\n",
        "# --- New Visualization for Comparing L1 and L2 Weights ---\n",
        "feature_names = ['X1', 'X2', 'X1^2', 'X2^2', 'X1*X2']\n",
        "non_reg_weights = nonlinear_model.weights.flatten()\n",
        "l2_weights = l2_model.weights.flatten()\n",
        "l1_weights = l1_model.weights.flatten()\n",
        "\n",
        "x = np.arange(len(feature_names))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
        "\n",
        "# Plot Non-regularized weights\n",
        "rects1 = ax1.bar(x, non_reg_weights, width, label='Non-Regularized', color='tomato')\n",
        "ax1.set_ylabel('Weight Value')\n",
        "ax1.set_title('Non-Regularized Weights')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(feature_names, rotation=45, ha=\"right\")\n",
        "ax1.axhline(0, color='grey', linewidth=0.8)\n",
        "ax1.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Plot L2 weights\n",
        "rects2 = ax2.bar(x, l2_weights, width, label='L2 Weights', color='royalblue')\n",
        "ax2.set_title('L2 (Ridge) Regularization Weights')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(feature_names, rotation=45, ha=\"right\")\n",
        "ax2.axhline(0, color='grey', linewidth=0.8)\n",
        "ax2.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Plot L1 weights\n",
        "rects3 = ax3.bar(x, l1_weights, width, label='L1 Weights', color='seagreen')\n",
        "ax3.set_title('L1 (Lasso) Regularization Weights')\n",
        "ax3.set_xticks(x)\n",
        "ax3.set_xticklabels(feature_names, rotation=45, ha=\"right\")\n",
        "ax3.axhline(0, color='grey', linewidth=0.8)\n",
        "ax3.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "fig.suptitle('Side-by-Side Comparison of Model Weights', fontsize=16)\n",
        "if not os.environ.get('HIDE_PLOTS', False): plt.savefig(\"submission_4b.png\") # Save the plot\n",
        "\n",
        "if not os.environ.get('HIDE_PLOTS', False): plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ4QUk_ktIyo"
      },
      "source": [
        "# --- Part 3: Hyperparameter Tuning (Grid Search) ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JErED0RgyDim"
      },
      "source": [
        "We will search for the best learning_rate (alpha) and regularization strength (lambda) using the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5niJ-dWStJlH"
      },
      "outputs": [],
      "source": [
        "#TODO: (3) define a list of at least 3 learning rates and at least 3 lambda values you want to validate\n",
        "learning_rates = []\n",
        "lambda_values = []\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "# Array to store the results for visualization\n",
        "grid_search_results = np.zeros((len(learning_rates), len(lambda_values)))\n",
        "\n",
        "\n",
        "# Transform validation set once\n",
        "X_val_poly = nonlinear_model.transform(X_val)\n",
        "\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    for j, lmbda in enumerate(lambda_values):\n",
        "        print(f\"Training with lr={lr}, lambda={lmbda}...\")\n",
        "        model = LogisticRegression(learning_rate=lr, n_iterations=5000,\n",
        "                                   regularization='L2', lambda_val=lmbda, poly_features=True)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        predictions = model.predict(X_val_poly)\n",
        "        accuracy = np.mean(predictions == y_val)\n",
        "        grid_search_results[i, j] = accuracy\n",
        "        print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_params = {'learning_rate': lr, 'lambda_val': lmbda}\n",
        "            best_model = model\n",
        "\n",
        "print(\"\\n--- Grid Search Results ---\")\n",
        "print(f\"Best parameters found: {best_params}\")\n",
        "print(f\"Best validation accuracy: {best_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kNjxjRVtP2m"
      },
      "outputs": [],
      "source": [
        "# Evaluate the best model on the unseen test set\n",
        "print(\"\\nEvaluating best model on the test set...\")\n",
        "X_test_poly = best_model.transform(X_test)\n",
        "test_predictions = best_model.predict(X_test_poly)\n",
        "test_accuracy = np.mean(test_predictions == y_test)\n",
        "print(f\"Test Set Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "plot_decision_boundary(best_model, X, y, f\"Best Model (lr={best_params['learning_rate']}, lambda={best_params['lambda_val']})\", 'submission_4c.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZMHoG1etSWj"
      },
      "source": [
        "# --- Part 4: Gradient Descent Variants ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI6PmeBytUYH"
      },
      "outputs": [],
      "source": [
        "X_train_poly = nonlinear_model.transform(X_train) # Pre-transform for this part\n",
        "\n",
        "def run_gd_variant(optimizer='batch', batch_size=32, n_epochs=100):\n",
        "    n_samples, n_features = X_train_poly.shape\n",
        "    weights = np.zeros((n_features, 1))\n",
        "    bias = 0\n",
        "    costs = []\n",
        "\n",
        "    if optimizer == 'batch':\n",
        "        iterations_per_epoch = 1\n",
        "    elif optimizer == 'stochastic':\n",
        "        iterations_per_epoch = n_samples\n",
        "    elif optimizer == 'mini-batch':\n",
        "        iterations_per_epoch = int(np.ceil(n_samples / batch_size))\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_cost = 0\n",
        "\n",
        "        # Shuffle data at the start of each epoch\n",
        "        permutation = np.random.permutation(n_samples)\n",
        "        X_shuffled = X_train_poly[permutation]\n",
        "        y_shuffled = y_train[permutation]\n",
        "\n",
        "        for i in range(0, n_samples, batch_size if optimizer != 'stochastic' else 1):\n",
        "            # TODO: (4) fix X_batch and y_batch below for each of these three cases\n",
        "            if optimizer == 'stochastic':\n",
        "                 # Hint: For SGD, the batch size is 1. Slice a single sample (row\n",
        "                 X_batch = None\n",
        "                 y_batch = None\n",
        "            elif optimizer == 'mini-batch':\n",
        "                 # Hint: For Mini-batch GD, slice a chunk of `batch_size` samples\n",
        "                 X_batch = None\n",
        "                 y_batch = None\n",
        "            else: # batch\n",
        "                 # Hint: For Batch GD, the \"batch\" is the entire training set.\n",
        "                 X_batch = None\n",
        "                 y_batch = None\n",
        "\n",
        "            m_batch = len(y_batch)\n",
        "            if m_batch == 0: continue\n",
        "\n",
        "            linear_model = np.dot(X_batch, weights) + bias\n",
        "            y_pred = 1 / (1 + np.exp(-linear_model))\n",
        "\n",
        "            cost = -1/m_batch * np.sum(y_batch * np.log(y_pred) + (1 - y_batch) * np.log(1 - y_pred))\n",
        "            epoch_cost += cost\n",
        "\n",
        "            dw = (1 / m_batch) * np.dot(X_batch.T, (y_pred - y_batch))\n",
        "            db = (1 / m_batch) * np.sum(y_pred - y_batch)\n",
        "\n",
        "            weights -= 0.1 * dw\n",
        "            bias -= 0.1 * db\n",
        "\n",
        "        costs.append(epoch_cost / iterations_per_epoch)\n",
        "\n",
        "    return costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSUdd0RstVzN"
      },
      "outputs": [],
      "source": [
        "# Run the optimizers\n",
        "cost_batch = run_gd_variant(optimizer='batch', n_epochs=500)\n",
        "cost_stochastic = run_gd_variant(optimizer='stochastic', n_epochs=500)\n",
        "cost_mini_batch = run_gd_variant(optimizer='mini-batch', batch_size=32, n_epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR_5S7ojtX8x"
      },
      "outputs": [],
      "source": [
        "# Visualize the cost functions\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(cost_batch, label='Batch GD', linewidth=2)\n",
        "plt.plot(cost_stochastic, label='Stochastic GD (SGD)', alpha=0.8)\n",
        "plt.plot(cost_mini_batch, label='Mini-batch GD', linestyle='--')\n",
        "plt.xlabel(\"Epochs (or Iterations for Batch GD)\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.title(\"Cost Function Convergence for Different GD Variants\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "if not os.environ.get('HIDE_PLOTS', False): plt.savefig(\"submission_4d.png\") # Save the plot\n",
        "if not os.environ.get('HIDE_PLOTS', False): plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
